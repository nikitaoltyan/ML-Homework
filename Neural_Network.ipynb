{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Neural Network.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBaLCXp5Fo4w"
      },
      "source": [
        "# Задание 2.1 - Нейронные сети\n",
        "\n",
        "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
        "\n",
        "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
        "\n",
        "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tFtg3qOEFo4z",
        "outputId": "2c528d37-59b7-439c-e316-64740998dfc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Запустите эту ячейку если работаете в Colab\n",
        "!git clone https://github.com/balezz/tad_ml_dl_2021.git\n",
        "%cd tad_ml_dl_2021/data\n",
        "! wget -c http://ufldl.stanford.edu/housenumbers/train_32x32.mat http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
        "%cd ../Assignment3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tad_ml_dl_2021'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Total 92 (delta 0), reused 0 (delta 0), pack-reused 92\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n",
            "/content/tad_ml_dl_2021/data\n",
            "--2021-03-07 14:28:28--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  19.9MB/s    in 14s     \n",
            "\n",
            "2021-03-07 14:28:42 (12.7 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2021-03-07 14:28:42--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Reusing existing connection to ufldl.stanford.edu:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  20.3MB/s    in 3.0s    \n",
            "\n",
            "2021-03-07 14:28:45 (20.3 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n",
            "FINISHED --2021-03-07 14:28:45--\n",
            "Total wall clock time: 17s\n",
            "Downloaded: 2 files, 235M in 17s (14.1 MB/s)\n",
            "/content/tad_ml_dl_2021/Assignment3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iDC5k_NFo41"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynpU8ln-Fo41"
      },
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
        "from layers import FullyConnectedLayer, ReLULayer\n",
        "from model import TwoLayerNet\n",
        "from trainer import Trainer, Dataset\n",
        "from optim import SGD, MomentumSGD\n",
        "from metrics import multiclass_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnosgPfOFo42"
      },
      "source": [
        "# Загружаем данные\n",
        "\n",
        "И разделяем их на training и validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGVDAxtKFo42"
      },
      "source": [
        "def prepare_for_neural_network(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    return train_flat, test_flat\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"../data\", max_train=10000, max_test=1000)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lUb1w_2Fo42"
      },
      "source": [
        "# Перед началом работы полезно посмотреть на данные.\n",
        "# Отобразим пример из каждого класса.\n",
        "classes = [str(i) for i in range(10)]\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(train_y == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(train_X[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk9zwWbAFo43"
      },
      "source": [
        "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sjV_-XyFo43"
      },
      "source": [
        "# Как всегда, начинаем с кирпичиков\n",
        "\n",
        "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
        "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
        "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
        "\n",
        "Начнем с ReLU, у которого параметров нет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Sb0XgJFo43"
      },
      "source": [
        "# TODO: Implement ReLULayer layer in layers.py\n",
        "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
        "\n",
        "X = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "\n",
        "assert check_layer_gradient(ReLULayer(), X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf-anmQ9Fo44"
      },
      "source": [
        "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
        "\n",
        "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
        "\n",
        "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9bjui6fUFo44"
      },
      "source": [
        "# TODO: Implement FullyConnected layer forward and backward methods\n",
        "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
        "# TODO: Implement storing gradients for W and B\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1hd8vyCFo44"
      },
      "source": [
        "## Создаем нейронную сеть\n",
        "\n",
        "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
        "\n",
        "Не забудьте реализовать очистку градиентов в начале функции."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_2wmpCpFo45"
      },
      "source": [
        "# TODO: In model.py, implement compute_loss_and_gradients function\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
        "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "\n",
        "# TODO Now implement backward pass and aggregate all of the params\n",
        "check_model_gradient(model, train_X[:2], train_y[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndd_kq3sFo45"
      },
      "source": [
        "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S9wdy01Fo45"
      },
      "source": [
        "# TODO Now implement l2 regularization in the forward and backward pass\n",
        "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
        "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
        "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
        "\n",
        "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qvAlMGKFo46"
      },
      "source": [
        "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
        "\n",
        "Какое значение точности мы ожидаем увидеть до начала тренировки?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sxB4C2WFo46"
      },
      "source": [
        "# Finally, implement predict function!\n",
        "\n",
        "# TODO: Implement predict function\n",
        "# What would be the value we expect?\n",
        "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqZvjIEFo46"
      },
      "source": [
        "# Допишем код для процесса тренировки\n",
        "\n",
        "Если все реализовано корректно, значение функции ошибки должно уменьшаться в некоторых эпохах. Не беспокойтесь пока про validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4EinHK0Fo46"
      },
      "source": [
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
        "\n",
        "# TODO Implement missing pieces in Trainer.fit function\n",
        "# You should expect loss to go down every epoch, even if it's slow\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goB4vZncFo47"
      },
      "source": [
        "# Улучшаем процесс тренировки\n",
        "\n",
        "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luIKmnSPFo47"
      },
      "source": [
        "## Уменьшение скорости обучения (learning rate decay)\n",
        "\n",
        "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
        "\n",
        "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
        "\n",
        "В нашем случае N будет равным 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XOolmI6Fo47"
      },
      "source": [
        "# TODO Implement learning rate decay inside Trainer.fit method\n",
        "# Decay should happen once per epoch\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
        "\n",
        "initial_learning_rate = trainer.learning_rate\n",
        "loss_history, train_history, val_history = trainer.fit()\n",
        "\n",
        "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
        "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbomN9sjFo48"
      },
      "source": [
        "# Накопление импульса (Momentum SGD)\n",
        "\n",
        "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
        "\n",
        "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
        "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
        "\n",
        "```\n",
        "velocity = momentum * velocity - learning_rate * gradient \n",
        "w = w + velocity\n",
        "```\n",
        "\n",
        "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
        "\n",
        "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
        "http://cs231n.github.io/neural-networks-3/#sgd  \n",
        "https://distill.pub/2017/momentum/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbWVPZmFo49"
      },
      "source": [
        "# TODO: Implement MomentumSGD.update function in optim.py\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "# You should see even better results than before!\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoHlRdSWFo49"
      },
      "source": [
        "# Ну что, давайте уже тренировать сеть!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO3UAQ28Fo4-"
      },
      "source": [
        "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
        "\n",
        "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
        "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
        "\n",
        "Если этого не происходит, то где-то была допущена ошибка!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-PhaISlFo4-"
      },
      "source": [
        "data_size = 15\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
        "\n",
        "# You should expect this to reach 1.0 training accuracy \n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKUV36CFo4-"
      },
      "source": [
        "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
        "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
        "Найдите их!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qEy_JzCFo4-"
      },
      "source": [
        "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
        "\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94kTz_u4Fo4_"
      },
      "source": [
        "# Итак, основное мероприятие!\n",
        "\n",
        "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
        "\n",
        "Добейтесь точности лучше **60%** на validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QfmAea3JFo4_"
      },
      "source": [
        "# Let's train the best one-hidden-layer network we can\n",
        "\n",
        "learning_rates = 1e-4\n",
        "reg_strength = 1e-3\n",
        "learning_rate_decay = 0.999\n",
        "hidden_layer_size = 128\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = None\n",
        "\n",
        "loss_history = []\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "# TODO find the best hyperparameters to train the network\n",
        "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
        "# You should expect to get to at least 40% of valudation accuracy\n",
        "# Save loss/train/history of the best classifier to the variables above\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w9o3nQ-Fo4_"
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.title(\"Train/validation accuracy\")\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lg_1gpdFo4_"
      },
      "source": [
        "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXUDSp_1Fo4_"
      },
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}